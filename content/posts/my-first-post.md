---
title: "A Review on Biomedical Research Paper Embedding"
date: 2025-02-03
tags: ["LLM", "Embedding", "Survey"]
draft: true
---

Efficient document embedding is a crucial step in many natural language processing tasks, such as information retrieval, question answering, and semantic search. By encoding documents into dense vectors, these tasks can be performed more efficiently and effectively. When it comes to the embedding of research papers, the common approach is to use the title and abstract as the input for efficiency. This is also the common practice in the biomedical systematic review. In this article, I will review the recent works on the embedding of research papers, especially the biomedical ones.

# SPECTER
SPECTER (Cohan et al., 2020) is a pre-trained language model specifically designed for scientific document embedding. To provide the linkage between articles, SPECTER firstly proposed SciDoc, a dataset of 1.2M scientific documents with citation links. Then, SPECTER was pre-trained on SciDoc with a masked language modeling objective. SPECTER used the same architecture as SciBERT (Beltagy et al., 2019), as shown in Figure 1. 
![SPECTER Architecture](/static/images/ReviewOnEmbeddingPics/SpecterArchitecture.png)
*Figure 1: The architecture of SPECTER model. Image Source: [Cohan et al., 2020](https://arxiv.org/pdf/2004.07180.pdf)*

When constructing the negative samples, SPECTER proposed a hard negative sampling strategy. The hard negative samples are generated by the following steps:
1. P1 cites P2.
2. P2 cites P3
3. P1 does not cite P3.
4. Then P1 and P3 are used to construct a hard negative sample.



# References

Cohan, A., Feldman, S., Beltagy, I., Downey, D., & Weld, D. S. (2020). Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180.

Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676.